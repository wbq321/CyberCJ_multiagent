"""
Smart Queue Manager for CyberCJ - Handles 30 concurrent users intelligently
"""

import time
import threading
from collections import defaultdict, deque
from functools import wraps
from flask import request, jsonify
import logging

logger = logging.getLogger(__name__)

class SmartQueueManager:
    def __init__(self):
        # Rate limiting
        self.request_counts = defaultdict(list)
        self.ai_queue = deque()
        self.ai_processing = 0
        self.max_ai_concurrent = 3  # Maximum AI requests processing simultaneously
        
        # User session tracking
        self.active_sessions = defaultdict(dict)
        self.queue_lock = threading.Lock()
        
    def rate_limit_check(self, client_ip, max_requests=15, window=60):
        """Check if user exceeds rate limit"""
        now = time.time()
        
        # Clean old requests
        self.request_counts[client_ip] = [
            req_time for req_time in self.request_counts[client_ip]
            if now - req_time < window
        ]
        
        return len(self.request_counts[client_ip]) < max_requests
    
    def add_request(self, client_ip):
        """Record a new request"""
        self.request_counts[client_ip].append(time.time())
    
    def can_process_ai_request(self):
        """Check if we can process another AI request"""
        with self.queue_lock:
            return self.ai_processing < self.max_ai_concurrent
    
    def start_ai_processing(self):
        """Mark start of AI processing"""
        with self.queue_lock:
            self.ai_processing += 1
    
    def finish_ai_processing(self):
        """Mark end of AI processing"""
        with self.queue_lock:
            self.ai_processing = max(0, self.ai_processing - 1)
    
    def get_queue_status(self):
        """Get current queue status"""
        with self.queue_lock:
            return {
                'ai_processing': self.ai_processing,
                'queue_length': len(self.ai_queue),
                'estimated_wait': len(self.ai_queue) * 10  # Estimate 10s per request
            }

# Global queue manager instance
queue_manager = SmartQueueManager()

def smart_rate_limit(max_requests=15, window=60):
    """Smart rate limiting decorator"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Get client IP
            client_ip = request.environ.get('HTTP_X_FORWARDED_FOR', 
                                           request.environ.get('REMOTE_ADDR', 'unknown'))
            
            # Check rate limit
            if not queue_manager.rate_limit_check(client_ip, max_requests, window):
                return jsonify({
                    'error': 'è¯·æ±‚è¿‡äºŽé¢‘ç¹ï¼Œè¯·ç¨åŽå†è¯• ðŸ•',
                    'retry_after': window,
                    'message': 'ä¸ºäº†ä¿è¯æ‰€æœ‰ç”¨æˆ·çš„ä½“éªŒï¼Œè¯·é€‚åº¦ä½¿ç”¨AIèŠå¤©åŠŸèƒ½'
                }), 429
            
            # Record request
            queue_manager.add_request(client_ip)
            
            return func(*args, **kwargs)
        return wrapper
    return decorator

def ai_queue_control(func):
    """AI request queue control decorator"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # Check if we can process immediately
        if not queue_manager.can_process_ai_request():
            status = queue_manager.get_queue_status()
            return jsonify({
                'error': 'AIç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åŽé‡è¯• ðŸ¤–',
                'queue_status': status,
                'message': f'å½“å‰æœ‰ {status["ai_processing"]} ä¸ªç”¨æˆ·åœ¨ä½¿ç”¨AIï¼Œé¢„è®¡ç­‰å¾…æ—¶é—´ {status["estimated_wait"]} ç§’'
            }), 503
        
        try:
            # Start processing
            queue_manager.start_ai_processing()
            result = func(*args, **kwargs)
            return result
        finally:
            # Always finish processing
            queue_manager.finish_ai_processing()
    
    return wrapper

def get_smart_response_for_busy_system():
    """Get helpful response when system is busy"""
    tips = [
        "ðŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥å…ˆæµè§ˆå­¦ä¹ æ¨¡å—ï¼Œç¨åŽå†å°è¯•AIèŠå¤©",
        "ðŸ“š å»ºè®®ï¼šæŸ¥çœ‹è¯¾ç¨‹å†…å®¹å’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œè¿™äº›ä¸éœ€è¦AIå¤„ç†",
        "â° å°è´´å£«ï¼šAIå“åº”éœ€è¦æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ä¸è¦é‡å¤ç‚¹å‡»",
        "ðŸŽ¯ å»ºè®®ï¼šå°è¯•æå‡ºæ›´å…·ä½“çš„é—®é¢˜ï¼ŒAIèƒ½ç»™å‡ºæ›´å¥½çš„å›žç­”"
    ]
    return {
        'response': 'ç³»ç»Ÿå½“å‰ç”¨æˆ·è¾ƒå¤šï¼Œä¸ºä¿è¯æœåŠ¡è´¨é‡ï¼Œè¯·ç¨åŽå†è¯•ã€‚',
        'tip': tips[int(time.time()) % len(tips)],
        'suggested_actions': [
            'æµè§ˆè¯¾ç¨‹æ¨¡å—',
            'æŸ¥çœ‹æ¡ˆä¾‹ç ”ç©¶', 
            'å¤ä¹ çŸ¥è¯†æ£€æŸ¥',
            'ç¨åŽå†è¯•AIèŠå¤©'
        ]
    }